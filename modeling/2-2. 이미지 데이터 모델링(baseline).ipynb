{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227784d7-6e2f-4431-bd05-d16b03103ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\train, 데이터 개수: 35375\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\val, 데이터 개수: 5052\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\test, 데이터 개수: 5052\n",
      "Using device: cuda\n",
      "Epoch 1/30, Train Loss: 0.6063, Val Loss: 2.0877, Val Accuracy: 54.55%\n",
      "Epoch 2/30, Train Loss: 0.5265, Val Loss: 1.8640, Val Accuracy: 59.01%\n",
      "Epoch 3/30, Train Loss: 0.5422, Val Loss: 1.5216, Val Accuracy: 65.91%\n",
      "Epoch 4/30, Train Loss: 0.4856, Val Loss: 1.3996, Val Accuracy: 69.14%\n",
      "Epoch 5/30, Train Loss: 0.4728, Val Loss: 1.7008, Val Accuracy: 66.65%\n",
      "Epoch 6/30, Train Loss: 0.4334, Val Loss: 1.7246, Val Accuracy: 67.18%\n",
      "Epoch 7/30, Train Loss: 0.4237, Val Loss: 2.0633, Val Accuracy: 64.94%\n",
      "Epoch 8/30, Train Loss: 0.4300, Val Loss: 1.9222, Val Accuracy: 67.04%\n",
      "Epoch 9/30, Train Loss: 0.4295, Val Loss: 2.0269, Val Accuracy: 66.15%\n",
      "Epoch 10/30, Train Loss: 0.4249, Val Loss: 1.9699, Val Accuracy: 66.63%\n",
      "Epoch 11/30, Train Loss: 0.4100, Val Loss: 1.8663, Val Accuracy: 68.09%\n",
      "Epoch 12/30, Train Loss: 0.4075, Val Loss: 2.1114, Val Accuracy: 66.47%\n",
      "Epoch 13/30, Train Loss: 0.4056, Val Loss: 2.0955, Val Accuracy: 66.51%\n",
      "Epoch 14/30, Train Loss: 0.4051, Val Loss: 2.2327, Val Accuracy: 65.62%\n",
      "Epoch 15/30, Train Loss: 0.4032, Val Loss: 2.1429, Val Accuracy: 67.22%\n",
      "Epoch 16/30, Train Loss: 0.3998, Val Loss: 2.2502, Val Accuracy: 65.70%\n",
      "Epoch 17/30, Train Loss: 0.3977, Val Loss: 2.2309, Val Accuracy: 65.76%\n",
      "Epoch 18/30, Train Loss: 0.3975, Val Loss: 2.3022, Val Accuracy: 65.52%\n",
      "Epoch 19/30, Train Loss: 0.3984, Val Loss: 2.3193, Val Accuracy: 65.54%\n",
      "Epoch 20/30, Train Loss: 0.3971, Val Loss: 2.3253, Val Accuracy: 65.52%\n",
      "Epoch 21/30, Train Loss: 0.3954, Val Loss: 2.3397, Val Accuracy: 65.40%\n",
      "Epoch 22/30, Train Loss: 0.3947, Val Loss: 2.3381, Val Accuracy: 65.48%\n",
      "Epoch 23/30, Train Loss: 0.3950, Val Loss: 2.3697, Val Accuracy: 65.22%\n",
      "Epoch 24/30, Train Loss: 0.3940, Val Loss: 2.3766, Val Accuracy: 65.22%\n",
      "Epoch 25/30, Train Loss: 0.3928, Val Loss: 2.3968, Val Accuracy: 65.22%\n",
      "Epoch 26/30, Train Loss: 0.3935, Val Loss: 2.3887, Val Accuracy: 65.26%\n",
      "Epoch 27/30, Train Loss: 0.3923, Val Loss: 2.3792, Val Accuracy: 65.32%\n",
      "Epoch 28/30, Train Loss: 0.3938, Val Loss: 2.3850, Val Accuracy: 65.26%\n",
      "Epoch 29/30, Train Loss: 0.3929, Val Loss: 2.3963, Val Accuracy: 65.16%\n",
      "Epoch 30/30, Train Loss: 0.3933, Val Loss: 2.3925, Val Accuracy: 65.22%\n",
      "Test Loss: 2.6985, Test Accuracy: 56.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. 데이터 전처리 및 로더 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".bin\"):\n",
    "                    bin_path = os.path.join(root, file)\n",
    "                    json_path = bin_path.replace(\".bin\", \".json\")\n",
    "\n",
    "                    try:\n",
    "                        # BIN 파일 읽기 (120x160 이미지)\n",
    "                        bin_data = np.load(bin_path, allow_pickle=True).astype(np.float32)\n",
    "                        bin_data = bin_data.reshape((120, 160))\n",
    "                    except (UnicodeDecodeError, ValueError):\n",
    "                        print(f\"[Error] BIN 파일 읽기 실패: {bin_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # JSON 파일에서 state 값 읽기\n",
    "                    if os.path.exists(json_path):\n",
    "                        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            state = json_data.get(\"annotations\", [{}])[0].get(\"tagging\", [{}])[0].get(\"state\", \"N/A\")\n",
    "                    else:\n",
    "                        print(f\"[Warning] JSON 파일을 찾을 수 없습니다: {json_path}\")\n",
    "                        continue\n",
    "\n",
    "                    self.data.append(bin_data)\n",
    "                    self.labels.append(state)\n",
    "\n",
    "        # Label Encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        print(f\"폴더: {root_folder}, 데이터 개수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # PyTorch 텐서 변환 및 채널 추가\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # (1, 120, 160)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size=32):\n",
    "    train_dataset = CustomDataset(os.path.join(data_folder, \"train\"))\n",
    "    val_dataset = CustomDataset(os.path.join(data_folder, \"val\"))\n",
    "    test_dataset = CustomDataset(os.path.join(data_folder, \"test\"))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 2. 모델 정의\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vit = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate  # Transformer 내부 드롭아웃 적용\n",
    "        )\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_dim_h // patch_size) * (img_dim_w // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # 추가 드롭아웃 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x).flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        x = patches + self.pos_embedding\n",
    "        x = self.vit(x, x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.dropout(x.mean(dim=1))  # 드롭아웃 적용\n",
    "        return x  # [batch_size, embed_dim]\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, num_classes, dropout_rate=0.5):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate  # 전달\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Classifier 레이어에도 드롭아웃 추가\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# 4. 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_loss / len(test_loader), test_accuracy\n",
    "\n",
    "\n",
    "# 5. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\"\n",
    "    batch_size = 16\n",
    "    train_loader, val_loader, test_loader = load_data(data_folder, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ViTClassifier(\n",
    "        img_dim_h=120, img_dim_w=160, patch_size=16, embed_dim=128,\n",
    "        num_heads=4, depth=8, num_classes=4, dropout_rate=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "\n",
    "    test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f85084-204d-4b20-aa53-ca8bd581867b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddf34e7-7fd1-45d4-9a02-6ebd1803a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\train, 데이터 개수: 35375\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\val, 데이터 개수: 5052\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\test, 데이터 개수: 5052\n",
      "Using device: cuda\n",
      "Epoch 1/30, Train Loss: 0.5692, Val Loss: 1.5499, Val Accuracy: 59.86%\n",
      "Epoch 2/30, Train Loss: 0.4870, Val Loss: 1.5847, Val Accuracy: 60.43%\n",
      "Epoch 3/30, Train Loss: 0.4626, Val Loss: 1.8631, Val Accuracy: 61.18%\n",
      "Epoch 4/30, Train Loss: 0.4578, Val Loss: 1.6939, Val Accuracy: 59.16%\n",
      "Epoch 5/30, Train Loss: 0.4392, Val Loss: 1.8795, Val Accuracy: 56.16%\n",
      "Epoch 6/30, Train Loss: 0.4078, Val Loss: 2.0049, Val Accuracy: 58.69%\n",
      "Epoch 7/30, Train Loss: 0.4013, Val Loss: 1.8121, Val Accuracy: 57.92%\n",
      "Epoch 8/30, Train Loss: 0.3971, Val Loss: 1.9878, Val Accuracy: 57.82%\n",
      "Epoch 9/30, Train Loss: 0.3964, Val Loss: 2.2613, Val Accuracy: 57.07%\n",
      "Epoch 10/30, Train Loss: 0.3951, Val Loss: 2.2263, Val Accuracy: 56.27%\n",
      "Epoch 11/30, Train Loss: 0.3789, Val Loss: 2.1311, Val Accuracy: 56.20%\n",
      "Epoch 12/30, Train Loss: 0.3741, Val Loss: 2.2419, Val Accuracy: 55.88%\n",
      "Epoch 13/30, Train Loss: 0.3730, Val Loss: 2.1146, Val Accuracy: 57.64%\n",
      "Epoch 14/30, Train Loss: 0.3695, Val Loss: 2.2104, Val Accuracy: 56.24%\n",
      "Epoch 15/30, Train Loss: 0.3695, Val Loss: 2.2145, Val Accuracy: 55.70%\n",
      "Epoch 16/30, Train Loss: 0.3617, Val Loss: 2.1275, Val Accuracy: 56.87%\n",
      "Epoch 17/30, Train Loss: 0.3604, Val Loss: 2.1438, Val Accuracy: 56.61%\n",
      "Epoch 18/30, Train Loss: 0.3590, Val Loss: 2.0550, Val Accuracy: 57.76%\n",
      "Epoch 19/30, Train Loss: 0.3582, Val Loss: 2.1217, Val Accuracy: 56.75%\n",
      "Epoch 20/30, Train Loss: 0.3559, Val Loss: 2.1406, Val Accuracy: 57.01%\n",
      "Epoch 21/30, Train Loss: 0.3568, Val Loss: 2.1744, Val Accuracy: 56.51%\n",
      "Epoch 22/30, Train Loss: 0.3541, Val Loss: 2.1220, Val Accuracy: 57.03%\n",
      "Epoch 23/30, Train Loss: 0.3537, Val Loss: 2.1568, Val Accuracy: 56.53%\n",
      "Epoch 24/30, Train Loss: 0.3532, Val Loss: 2.1976, Val Accuracy: 56.45%\n",
      "Epoch 25/30, Train Loss: 0.3535, Val Loss: 2.1549, Val Accuracy: 56.61%\n",
      "Epoch 26/30, Train Loss: 0.3533, Val Loss: 2.1582, Val Accuracy: 56.55%\n",
      "Epoch 27/30, Train Loss: 0.3526, Val Loss: 2.1970, Val Accuracy: 56.25%\n",
      "Epoch 28/30, Train Loss: 0.3534, Val Loss: 2.1697, Val Accuracy: 56.39%\n",
      "Epoch 29/30, Train Loss: 0.3526, Val Loss: 2.1830, Val Accuracy: 56.37%\n",
      "Epoch 30/30, Train Loss: 0.3528, Val Loss: 2.1895, Val Accuracy: 56.22%\n",
      "Test Loss: 2.2619, Test Accuracy: 53.54%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2480  104    0    0]\n",
      " [ 863  225    0    0]\n",
      " [ 365  685    0    0]\n",
      " [  24  305    1    0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAHHCAYAAAAiSltoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVsklEQVR4nO3deVwU9f8H8NcusAsIu4jIpYgoguB9R+aVKOKdlmeKZ98UTCWPLG8zfmneeWXllaZmaql5IF6pWEqheOGFeXEoCAso587vD2JrhU2Wa2Hn9ewxj0c7+5mZ90wmb96fYySCIAggIiIi0ZIaOgAiIiIyLCYDREREIsdkgIiISOSYDBAREYkckwEiIiKRYzJAREQkckwGiIiIRI7JABERkcgxGSAiIhI5JgNEL7l16xa6du0KpVIJiUSCffv2ler57927B4lEgk2bNpXqeSuzjh07omPHjoYOg0i0mAxQhXTnzh3873//Q506dWBubg6FQoG2bdtixYoVePHiRZleOyAgAFFRUVi4cCG2bt2Kli1blun1ytOIESMgkUigUCgKfY63bt2CRCKBRCLBF198off5Hz9+jLlz5yIyMrIUoiWi8mJq6ACIXnbw4EG88847kMvlGD58OBo2bIisrCycOXMGU6dOxdWrV/HVV1+VybVfvHiB8PBwfPLJJwgKCiqTa7i6uuLFixcwMzMrk/O/iqmpKZ4/f479+/djwIABWt9t27YN5ubmyMjIKNa5Hz9+jHnz5qF27dpo2rRpkY87evRosa5HRKWDyQBVKDExMRg0aBBcXV1x/PhxODk5ab4LDAzE7du3cfDgwTK7/pMnTwAANjY2ZXYNiUQCc3PzMjv/q8jlcrRt2xbff/99gWRg+/bt6NGjB3788cdyieX58+ewtLSETCYrl+sRUeHYTUAVyqJFi5CWloZvvvlGKxHI5+7ujokTJ2o+5+TkYMGCBahbty7kcjlq166Njz/+GJmZmVrH1a5dGz179sSZM2fQunVrmJubo06dOtiyZYumzdy5c+Hq6goAmDp1KiQSCWrXrg0gr7ye/+//NnfuXEgkEq19oaGheOONN2BjYwMrKyt4enri448/1nyva8zA8ePH0a5dO1SpUgU2Njbo06cPrl+/Xuj1bt++jREjRsDGxgZKpRIjR47E8+fPdT/YlwwZMgSHDh1CcnKyZt+FCxdw69YtDBkypED7pKQkTJkyBY0aNYKVlRUUCgX8/f1x6dIlTZuTJ0+iVatWAICRI0dquhvy77Njx45o2LAhIiIi0L59e1haWmqey8tjBgICAmBubl7g/v38/FC1alU8fvy4yPdKRK/GZIAqlP3796NOnTp4/fXXi9R+zJgxmD17Npo3b45ly5ahQ4cOCAkJwaBBgwq0vX37Nt5++2106dIFS5YsQdWqVTFixAhcvXoVANCvXz8sW7YMADB48GBs3boVy5cv1yv+q1evomfPnsjMzMT8+fOxZMkS9O7dG2fPnv3P444dOwY/Pz8kJCRg7ty5CA4Oxrlz59C2bVvcu3evQPsBAwYgNTUVISEhGDBgADZt2oR58+YVOc5+/fpBIpFgz549mn3bt29H/fr10bx58wLt7969i3379qFnz55YunQppk6diqioKHTo0EHzg9nLywvz588HALz33nvYunUrtm7divbt22vOk5iYCH9/fzRt2hTLly9Hp06dCo1vxYoVqF69OgICApCbmwsAWL9+PY4ePYpVq1bB2dm5yPdKREUgEFUQKSkpAgChT58+RWofGRkpABDGjBmjtX/KlCkCAOH48eOafa6urgIA4fTp05p9CQkJglwuFz788EPNvpiYGAGAsHjxYq1zBgQECK6urgVimDNnjvDv/42WLVsmABCePHmiM+78a2zcuFGzr2nTpoK9vb2QmJio2Xfp0iVBKpUKw4cPL3C9UaNGaZ3zrbfeEqpVq6bzmv++jypVqgiCIAhvv/220LlzZ0EQBCE3N1dwdHQU5s2bV+gzyMjIEHJzcwvch1wuF+bPn6/Zd+HChQL3lq9Dhw4CAGHdunWFftehQwetfUeOHBEACJ9++qlw9+5dwcrKSujbt+8r75GI9MfKAFUYKpUKAGBtbV2k9r/88gsAIDg4WGv/hx9+CAAFxhZ4e3ujXbt2ms/Vq1eHp6cn7t69W+yYX5Y/1uCnn36CWq0u0jGxsbGIjIzEiBEjYGtrq9nfuHFjdOnSRXOf//b+++9rfW7Xrh0SExM1z7AohgwZgpMnTyIuLg7Hjx9HXFxcoV0EQN44A6k076+L3NxcJCYmarpA/vjjjyJfUy6XY+TIkUVq27VrV/zvf//D/Pnz0a9fP5ibm2P9+vVFvhYRFR2TAaowFAoFACA1NbVI7f/66y9IpVK4u7tr7Xd0dISNjQ3++usvrf21atUqcI6qVavi2bNnxYy4oIEDB6Jt27YYM2YMHBwcMGjQIOzates/E4P8OD09PQt85+XlhadPnyI9PV1r/8v3UrVqVQDQ6166d+8Oa2tr7Ny5E9u2bUOrVq0KPMt8arUay5YtQ7169SCXy2FnZ4fq1avj8uXLSElJKfI1a9SooddgwS+++AK2traIjIzEypUrYW9vX+RjiajomAxQhaFQKODs7IwrV67oddzLA/h0MTExKXS/IAjFvkZ+f3Y+CwsLnD59GseOHcOwYcNw+fJlDBw4EF26dCnQtiRKci/55HI5+vXrh82bN2Pv3r06qwIA8NlnnyE4OBjt27fHd999hyNHjiA0NBQNGjQocgUEyHs++vjzzz+RkJAAAIiKitLrWCIqOiYDVKH07NkTd+7cQXh4+Cvburq6Qq1W49atW1r74+PjkZycrJkZUBqqVq2qNfI+38vVBwCQSqXo3Lkzli5dimvXrmHhwoU4fvw4Tpw4Uei58+OMjo4u8N2NGzdgZ2eHKlWqlOwGdBgyZAj+/PNPpKamFjroMt/u3bvRqVMnfPPNNxg0aBC6du0KX1/fAs+kqIlZUaSnp2PkyJHw9vbGe++9h0WLFuHChQuldn4i+geTAapQpk2bhipVqmDMmDGIj48v8P2dO3ewYsUKAHllbgAFRvwvXboUANCjR49Si6tu3bpISUnB5cuXNftiY2Oxd+9erXZJSUkFjs1ffOfl6Y75nJyc0LRpU2zevFnrh+uVK1dw9OhRzX2WhU6dOmHBggX48ssv4ejoqLOdiYlJgarDDz/8gEePHmnty09aCkuc9DV9+nTcv38fmzdvxtKlS1G7dm0EBATofI5EVHxcdIgqlLp162L79u0YOHAgvLy8tFYgPHfuHH744QeMGDECANCkSRMEBATgq6++QnJyMjp06IDff/8dmzdvRt++fXVOWyuOQYMGYfr06XjrrbfwwQcf4Pnz51i7di08PDy0BtDNnz8fp0+fRo8ePeDq6oqEhASsWbMGNWvWxBtvvKHz/IsXL4a/vz98fHwwevRovHjxAqtWrYJSqcTcuXNL7T5eJpVKMXPmzFe269mzJ+bPn4+RI0fi9ddfR1RUFLZt24Y6depotatbty5sbGywbt06WFtbo0qVKmjTpg3c3Nz0iuv48eNYs2YN5syZo5nquHHjRnTs2BGzZs3CokWL9DofEb2CgWczEBXq5s2bwtixY4XatWsLMplMsLa2Ftq2bSusWrVKyMjI0LTLzs4W5s2bJ7i5uQlmZmaCi4uLMGPGDK02gpA3tbBHjx4FrvPylDZdUwsFQRCOHj0qNGzYUJDJZIKnp6fw3XffFZhaGBYWJvTp00dwdnYWZDKZ4OzsLAwePFi4efNmgWu8PP3u2LFjQtu2bQULCwtBoVAIvXr1Eq5du6bVJv96L09d3LhxowBAiImJ0flMBUF7aqEuuqYWfvjhh4KTk5NgYWEhtG3bVggPDy90SuBPP/0keHt7C6amplr32aFDB6FBgwaFXvPf51GpVIKrq6vQvHlzITs7W6vd5MmTBalUKoSHh//nPRCRfiSCoMeIIyIiIjI6HDNAREQkckwGiIiIRI7JABERkcgxGSAiIhI5JgNEREQix2SAiIhI5Cr1okNqtRqPHz+GtbV1qS6DSkRE5UMQBKSmpsLZ2VnzZsyykJGRgaysrBKfRyaTwdzcvBQiqlgqdTLw+PFjuLi4GDoMIiIqoQcPHqBmzZplcu6MjAxYWFcDcp6X+FyOjo6IiYkxuoSgUicD+e+9l3kHQGJS9NeiUvHdPPq5oUMQHXNZ4W8oJDIGqSoV3N1cNH+fl4WsrCwg5znk3gFASX5W5GYh7tpmZGVlMRmoSPK7BiQmMiYD5UShUBg6BNFhMkBiUC5dvabmJfpZIUiMd5hdpU4GiIiIikwCoCRJhxEPTWMyQERE4iCR5m0lOd5IGe+dERERUZGwMkBEROIgkZSwm8B4+wmYDBARkTiwm0An470zIiIiKhJWBoiISBzYTaATkwEiIhKJEnYTGHEx3XjvjIiIiIqElQEiIhIHdhPoxGSAiIjEgbMJdDLeOyMiIqIiYWWAiIjEgd0EOjEZICIicWA3gU5MBoiISBxYGdDJeNMcIiIiKhJWBoiISBzYTaATkwEiIhIHiaSEyQC7CYiIiMhIsTJARETiIJXkbSU53kgxGSAiInHgmAGdjPfOiIiIqEhYGSAiInHgOgM6MRkgIiJxYDeBTsZ7Z0RERFQkrAwQEZE4sJtAJyYDREQkDuwm0InJABERiQMrAzoZb5pDRERERcLKABERiQO7CXRiMkBEROLAbgKdjDfNISIioiJhZYCIiESihN0ERvz7M5MBIiISB3YT6GS8aQ4REREVCSsDREQkDhJJCWcTGG9lgMkAERGJA6cW6mS8d0ZERERFwmSgmCaP6IqwzVNx/+QXuHkkBN8tHgt3V3ud7X9YMQ7PLnyJ7h0aa+1v5l0L+9ZMwL3jixATtgi7VwaiYb0aWm0auDvjl68mIfbMMlw5sAAfDPMtk3uqbMIjb2P4tK/QtPcsOLWdiEOnL2t9LwgCFm34BU16z4JbpykYMHE17j5IKPRcmVk58A1YBKe2E3Hl5sPyCN+obdh1Co17z4Zj20nwHbEYEVfvGToko8bnXUT5AwhLshmpCpEMrF69GrVr14a5uTnatGmD33//3dAhvdLrzd3x9Q+n0XXUF+gX9CXMTE2wZ1UQLM1lBdqOG9wJglDwHFUsZNi9IhAP457Bd+QX8B+7FGnPM7B7VSBMTfL+01hXMcePXwbhQVwSOg3/HLNX7MP097oj4K22ZX2LFd7zF1nwdq+Bzz58u9DvV28Lwze7T+PzqQNwcMNkWJrLMDh4HTIyswu0XbDmJzjYKco6ZFHYczQCM5fvxfQx/ji5dToa1quB/hNW40lSqqFDM0p83nrI7yYoyWakDH5nO3fuRHBwMObMmYM//vgDTZo0gZ+fHxISCv8NrqJ454M1+P7Ab7hxNw5Xbj3C+HnfwcXJFk29XLTaNfSogcChbyJowXcFzlGvtiNsbaogZP0B3P4rATfuxmHRhkNwqKaAi5Nt3nW6tYTM1ARB87fhxt047AmNwFc7T2L8kE7lcp8VWWcfb3z0Xg9079CkwHeCIGDDrlOYFNAV3do1grd7Dayc9S7in6bg8K9RWm3Dwq/h1O/RmB3Ut5wiN25rth/H8L6vY2hvH9Sv44SlMwbB0lyG734ON3RoRonPWw+sDOhk8GRg6dKlGDt2LEaOHAlvb2+sW7cOlpaW+Pbbbw0dml4UVuYAgGeq55p9FnIzbFgwAlMX7UJCYsEs/fZf8UhMTsO7vV+HmakJzOVmeLePD27cjcX92CQAQKtGbjj3521k5+RqjgsLvw6P2o5QWluU8V1VXvcfJyIhUYV2LT00+xRWFmjm7YqLV2I0+54kqTD18x1YNetdWJqbGSJUo5KVnYPIGw/QsbWnZp9UKkWH1p64EBXzH0dScfB5U2kxaDKQlZWFiIgI+Pr+0wculUrh6+uL8PCCWW1mZiZUKpXWVhFIJBKEBL+N85F3cP1OrGb/Z8H98fvlGBw6HVXocWnPM9Hr/RUY4N8KsWeW4eGpJejs44UBE9cgN1cNALCvpihQ7sv/7FCNZW1dEv5+RtVtrbX2V7e1xpO/EzNBEDBx4XYM69sWTb1qlXuMxigxOQ25uepCnrsCCYkV4/9XY8LnrSd2E+hk0Dt7+vQpcnNz4eDgoLXfwcEBcXFxBdqHhIRAqVRqNhcXlwJtDOGLaQPgVdcJoz/ZqNnn374R2rX0wMdLd+s8zlxuhpUzh+K3S3fRZdQX6DZmKa7ficXO5eNgLudvqWXtm92nkfY8Ax8M62LoUIioPLCbQKdKtc7AjBkzEBwcrPmsUqkMnhAsmvoO/No1RPf3luNxQrJmf7uWHnCraYd7xxdrtd/y+RiER95Br/dX4G2/lqjlZIuuo5ZA+HuE4diZmxBzfBG6t2+MPaERSEhUFfrbLQDEM/PXyf7vZ/QkKRUOdkrN/idJqWjw92yNMxG3EHHlHlw7fah1bLcxS9CvSwusnPVu+QVsJKrZWMHERFpINUsFe1aySh2fN5UWgyYDdnZ2MDExQXx8vNb++Ph4ODo6Fmgvl8shl8vLK7xXWjT1HfTo2AS93l+B+48Ttb5bvvkotv50TmvfuR2f4ONlP+Lwr1cAABbmMqgFQZMIAPj7MyCV5mWgF6JiMHNcL5iaSJHzd9dBpzb1cfNeHFJSX5Tl7VVqtZyrwb6aAmcibqKhR00AQGp6Bv689hcC3noDAPDppH746L3ummPinqgwOHgt1s0LQPMGtQ0RdqUnMzNF0/ouOHUhGj065g3sVKvVOH3hJsa8097A0RkfPm/9SCQSSPhugkIZNBmQyWRo0aIFwsLC0LdvXwB5f5DDwsIQFBRkyNBe6YvpA/C2X0sMmfIV0p5nwL5a3m+iqrQMZGRmIyExtdBBgw/jnmkSh5O/3cD8D/rii+kD8NXOU5BKJZgU0BW5ubn49eJNAMDuwxcxbWx3rJo1FCu2hMKrrjP+N6gjPlm2p/xutoJKf56JmIdPNJ/vP07ElZsPYaOwRE1HW4wd0AHLNx+FW83qqOVcDZ9v+AUOdkp0a9cIAFDT0VbrfFUs8hLN2jXs4GxvU273YWzGD3kT4+dtRTOvWmjeoDbWfn8C6S8yMbTXa4YOzSjxeRcdkwHdDN5NEBwcjICAALRs2RKtW7fG8uXLkZ6ejpEjRxo6tP80+u28rPvg+kla+8fP24rvD/xWpHPc+iseg4PXY/pYfxz99kOo1QIu33yItz9Yo+kCUKVnoH/Ql1g8bQBObJmOxOQ0LP76EDbvPVuq91MZXbpxH/0nfKn5PHfVPgDAAP/WWDFzKAKHdsbzF1mYumgnVGkv0LpxHWxf8j7HY5Sxfl1b4GlyGj5bfxAJialo5FEDu1cGsmxdRvi8qTRIBKGw5XDK15dffonFixcjLi4OTZs2xcqVK9GmTZtXHqdSqaBUKiFvNBYSk4KL/VDpiz27wtAhiI65zMTQIRCVGZVKBYdqSqSkpEChKJsEJv9nhUWf1ZCYFX9KtpD9Ai9+CizTWA3F4JUBAAgKCqrw3QJERFS5sZtAN+OdNElERERFUiEqA0RERGWNlQHdmAwQEZEoMBnQjckAERGJApMB3ThmgIiISORYGSAiInGQ/L2V5HgjxWSAiIhEgd0EurGbgIiISORYGSAiIlHIewtxSSoDpRdLRcNkgIiIREGCEnYTGHE2wG4CIiIikWNlgIiIRIEDCHVjMkBEROLAqYU6sZuAiIhI5JgMEBGROPzdTVDcTd9ugpCQELRq1QrW1tawt7dH3759ER0drdUmIyMDgYGBqFatGqysrNC/f3/Ex8drtbl//z569OgBS0tL2NvbY+rUqcjJydFqc/LkSTRv3hxyuRzu7u7YtGmTXrEyGSAiIlEoSSJQnPEGp06dQmBgIM6fP4/Q0FBkZ2eja9euSE9P17SZPHky9u/fjx9++AGnTp3C48eP0a9fP833ubm56NGjB7KysnDu3Dls3rwZmzZtwuzZszVtYmJi0KNHD3Tq1AmRkZGYNGkSxowZgyNHjhT92QiCIOh1dxWISqWCUqmEvNFYSExkhg5HFGLPrjB0CKJjLjMxdAhEZUalUsGhmhIpKSlQKBRldg2lUolqQzdCKrMs9nnUWc+RuG1ksWN98uQJ7O3tcerUKbRv3x4pKSmoXr06tm/fjrfffhsAcOPGDXh5eSE8PByvvfYaDh06hJ49e+Lx48dwcHAAAKxbtw7Tp0/HkydPIJPJMH36dBw8eBBXrlzRXGvQoEFITk7G4cOHixQbKwNERETlICUlBQBga2sLAIiIiEB2djZ8fX01berXr49atWohPDwcABAeHo5GjRppEgEA8PPzg0qlwtWrVzVt/n2O/Db55ygKziYgIiJxKKXZBCqVSmu3XC6HXC7/z0PVajUmTZqEtm3bomHDhgCAuLg4yGQy2NjYaLV1cHBAXFycps2/E4H87/O/+682KpUKL168gIWFxStvjZUBIiIShdIaM+Di4gKlUqnZQkJCXnntwMBAXLlyBTt27Cjr2ywWVgaIiIj08ODBA60xA6+qCgQFBeHAgQM4ffo0atasqdnv6OiIrKwsJCcna1UH4uPj4ejoqGnz+++/a50vf7bBv9u8PAMhPj4eCoWiSFUBgJUBIiISidKqDCgUCq1NVzIgCAKCgoKwd+9eHD9+HG5ublrft2jRAmZmZggLC9Psi46Oxv379+Hj4wMA8PHxQVRUFBISEjRtQkNDoVAo4O3trWnz73Pkt8k/R1GwMkBERKJQ0uWI9T02MDAQ27dvx08//QRra2tNH79SqYSFhQWUSiVGjx6N4OBg2NraQqFQYMKECfDx8cFrr70GAOjatSu8vb0xbNgwLFq0CHFxcZg5cyYCAwM1Scj777+PL7/8EtOmTcOoUaNw/Phx7Nq1CwcPHixyrKwMEBERlYG1a9ciJSUFHTt2hJOTk2bbuXOnps2yZcvQs2dP9O/fH+3bt4ejoyP27Nmj+d7ExAQHDhyAiYkJfHx88O6772L48OGYP3++po2bmxsOHjyI0NBQNGnSBEuWLMHXX38NPz+/IsfKdQZIL1xnoPxxnQEyZuW5zoDDiK0lXmcgftOwMo3VUNhNQERE4sAXFenEbgIiIiKRY2WAiIhEobwHEFYmTAaIiEgUmAzoxmSAiIhEgcmAbhwzQEREJHKsDBARkThwNoFOTAaIiEgU2E2gG7sJiIiIRI6VASIiEgVWBnRjMkBERKIgQQmTASMeNMBuAiIiIpFjZYCIiESB3QS6MRkgIiJx4NRCnYwiGVj95WRYWlkbOgxRSM/KNXQIosNXGBNRWTOKZICIiOhV2E2gG5MBIiISBSYDujEZICIiUZBI8raSHG+sOLWQiIhI5FgZICIiUcirDJSkm6AUg6lgmAwQEZE4lLCbwJinFrKbgIiISORYGSAiIlHgbALdmAwQEZEocDaBbuwmICIiEjlWBoiISBSkUgmk0uL/ei+U4NiKjskAERGJArsJdGM3ARERkcixMkBERKLA2QS6MRkgIiJRYDeBbkwGiIhIFFgZ0I1jBoiIiESOlQEiIhIFVgZ0YzJARESiwDEDurGbgIiISORYGSAiIlGQoITdBEb8DmMmA0REJArsJtCN3QREREQix8oAERGJAmcT6MZkgIiIRIHdBLqxm4CIiEjkWBkgIiJRYDeBbkwGiIhIFNhNoBuTASIiEgVWBnTjmAEiIiKRY2WAiIjEoYTdBEa8ACGTASIiEgd2E+jGbgIiIiKRY2WAiIhEgbMJdGMyQEREosBuAt3YTUBERCRyrAwQEZEosJtANyYDREQkCuwm0I3dBERERCLHygAREYkCKwO6MRkoJWq1Gvt+OoPz568iJSUdNjZWaNu2EXr1fF3rD9Djx0+xe/dJRN98gNxcNZydqyFw/FuoVk0JANi85TCuXbuH5OQ0yOVmcHevgXfe7gQnp2qGurUKa+22Yzj6axTu3k+AXG6G5g1qY9p7PVGnlj0AIFmVjhWbjuDMxWg8jn8GWxsrdGnbEJNH+cPaykJzHvdOwQXOvXzWMPR8s1m53Yux2bDrFFZ9F4aERBUa1quBz6e+gxYNahs6LKPF5100HDOgm0GTgdOnT2Px4sWIiIhAbGws9u7di759+xoypGL75dB5nDz5J0aP6oEaNexw714cvvn2F1hYyNHFtyUAICHhGUL+7zu0a9cEffq8AQsLOR49fgozs3/+M7i6OuK1Nt6oVk2B9PQM/PTTGSxZuhOLPn8fUil7df7t90t38G7ftmjkWQu5ublY8vUvGDFtPQ5vnAZLCzkSElVIeJqCj97vDXdXBzyOf4ZZy3YjPlGF1fNGaJ3r8+mD0L51fc1nxb+SBdLPnqMRmLl8L5Z+NBAtGtbGuu9PoP+E1biwezaq21obOjyjw+dddKwM6GbQny7p6elo0qQJVq9ebcgwSsXt24/QtGk9NGniDjs7G7RsWR8NG9RGTEysps2ePafRuFFdDHinE1xdHWFvXxXNmtaDQlFF06Zjh6bw9KwFOzsbuLo64q232iMpSYWnT1MMcVsV2sZF/0P/bq3h4eYIL/ca+PyjwXgc/wxXbj4EAHi4OWH1/JHo/HoDuNawg0/zegge7Y/j4VeRk5urdS6FlQWq2yo0m1xmZohbMgprth/H8L6vY2hvH9Sv44SlMwbB0lyG734ON3RoRonPm0qDQSsD/v7+8Pf3N2QIpcbdvQZOnYpEXFwSHB1tcf9BPG7dfoiBAzsDANRqAZcu34G/fxssWboT9+/Hw85OiR7dfdC8uUeh58zMzMKZs5dhZ6eEra2iPG+nUkpNfwEAsFFY/kebDFhZmsPUxERr/9wVP+Ljxbvg4myLwb1ex9v+rY36t4CykpWdg8gbDzB5RFfNPqlUig6tPXEhKsaAkRknPm/9sJtAN44ZKCXd/X3w4kUWPpn5FaRSKdRqNfq91QE+rzUAAKSmpiMzMwu//HIe/d5qh3fe7oioK3exes0eTJs6BJ6etTTnOn78D/yw+wQyM7Ph6GiLKR8Ogqmpia5LE/LGbCz88ie0aOgGDzenQtskpaRh9dZQDOrpo7V/0shu8GlWD+bmZjhzMRpzlv+I5y8yEdC/fXmEblQSk9OQm6suUJ6ubqvArXvxBorKePF564fdBLpVqmQgMzMTmZmZms8qlcqA0Wi7cOE6zp+/ivfG9kaNGna4fz8B3+84phlIqFYLAIBmzeqha9fWAIBatRxw5/YjnDj5p1Yy8Npr3mjQoDaSk9Nw5MjvWLtuHz6eMUxrbAFpm7tiD27GxGLHqgmFfp+anoGxH30Nd1cHfDDCT+u7oOH//FbVoF5NvHiRhQ07TzIZICLRqFQj0kJCQqBUKjWbi4uLoUPS2PXDCXTv/hratPFGzZr2eP31hujapRUO/pLXb2dtbQkTEymcX5oV4ORUDUlJ2kmNpaU5HBxs4elZC+PHv4XY2CRE/HGz3O6lspm74kccD7+G75aNh1N1mwLfpz3PwKjpX6GKpRxrF4yE2SuqLE28XBH3JBmZWTllFLHxqmZjBRMTKZ4kpWrtf5Kkgn01dnWVNj5v/UjwT1dBsTZD30AZqlTJwIwZM5CSkqLZHjx4YOiQNLKysiF9qYQklUohCHkVAVNTE9Su7YS4uCStNnHxSZpphYXJO15ATjZ/ML1MEATMXfEjQs9E4bul4+BSyPTL1PQMjJi6HmamJli/cHSRBgZev/MISmsLyGWsxOhLZmaKpvVdcOpCtGafWq3G6Qs30aqRmwEjM0583vqRSiQl3oxVpfrbTi6XQy6XGzqMQjVt4o4DB8Nha6tAjRp2+Ot+PI4c/R3t3misadOtW2usW/cTPDxcUL++K65cuYtLl25j2rQhAICEJ8m48Pt1NGjgBmtrCzx7lopffjkPMzNTNG5c11C3VmHNWf4j9of9gXWfjkIVSzme/F1hsa5iDnO57O9EYB0yMrOx5OOhSHuegbTnGQAAW2Xeb1Rh567i6bNUNPN2hUxmirMXb2LttjCMHtDRgHdWuY0f8ibGz9uKZl610LxBbaz9/gTSX2RiaK/XDB2aUeLzptJg0GQgLS0Nt2/f1nyOiYlBZGQkbG1tUatWrf84suIZMqQL9u77Fd99dxSq1OewsbFCxw7N0Lt3W02bFs09MXyYHw7+ch7bvz8GR0dbBI5/Cx718ro7zExNcPPWA4Qeu4D09AwoFFXg6eGCjz8epjX9kPJs//kcAGDo5DVa+z+fPgj9u7XG1VsPcen6fQBA53c/02pz8vuZqOloC1NTKb7bdxafrf4JgiDAtYYdPh7XGwN78i/S4urXtQWeJqfhs/UHkZCYikYeNbB7ZSDL1mWEz7voyns2wavW0hkxYgQ2b96sdYyfnx8OHz6s+ZyUlIQJEyZg//79kEql6N+/P1asWAErKytNm8uXLyMwMBAXLlxA9erVMWHCBEybNk2/exPy69gGcPLkSXTq1KnA/oCAAGzatOmVx6tUKiiVSnx96hosrbi4RnloX8fe0CGITjUrmaFDICozKpUKDtWUSElJgUJRNglM/s+KN78Ig6lF8X+xynmRjuNTOhc51kOHDuHs2bNo0aIF+vXrV2gyEB8fj40bN2r2yeVyVK1aVfPZ398fsbGxWL9+PbKzszFy5Ei0atUK27dv19ybh4cHfH19MWPGDERFRWHUqFFYvnw53nvvvSLfm0ErAx07doQBcxEiIhIRqSRvK8nx+ijKWjpyuRyOjo6Ffnf9+nUcPnwYFy5cQMuWeSvZrlq1Ct27d8cXX3wBZ2dnbNu2DVlZWfj2228hk8nQoEEDREZGYunSpXolA5VqACEREZGhqVQqre3fU971dfLkSdjb28PT0xPjxo1DYmKi5rvw8HDY2NhoEgEA8PX1hVQqxW+//aZp0759e8hk/1QQ/fz8EB0djWfPnhU5DiYDREQkDpJ/Fh4qzpY/t9DFxUVrmntISEixwunWrRu2bNmCsLAwfP755zh16hT8/f2R+/dy6XFxcbC31+6aNTU1ha2tLeLi4jRtHBwctNrkf85vUxSVajYBERFRcZXWAMIHDx5ojRko7iy3QYMGaf69UaNGaNy4MerWrYuTJ0+ic+fOxQ+0GFgZICIi0oNCodDaSmvKe506dWBnZ6eZZefo6IiEhAStNjk5OUhKStKMM3B0dER8vPbS0/mfdY1FKAyTASIiEgVJKfxTlh4+fIjExEQ4OeW9X8XHxwfJycmIiIjQtDl+/DjUajXatGmjaXP69GlkZ2dr2oSGhsLT01NrVsKrMBkgIiJRyJ9NUJJNH2lpaYiMjERkZCSAf9bSuX//PtLS0jB16lScP38e9+7dQ1hYGPr06QN3d3f4+eW9P8XLywvdunXD2LFj8fvvv+Ps2bMICgrCoEGD4OzsDAAYMmQIZDIZRo8ejatXr2Lnzp1YsWIFgoOD9Xs2+t0aERERFcXFixfRrFkzNGvWDAAQHByMZs2aYfbs2TAxMcHly5fRu3dveHh4YPTo0WjRogV+/fVXrW6Hbdu2oX79+ujcuTO6d++ON954A1999ZXme6VSiaNHjyImJgYtWrTAhx9+iNmzZ+s1rRDgAEIiIhKJ8n6F8avW0jly5Mgrz2Fra6tZYEiXxo0b49dff9UrtpcVKRn4+eefi3zC3r17FzsYIiKislLeyxFXJkVKBv69fOJ/kUgkmvmRREREVDkUKRlQq9VlHQcREVGZKulriPkKYx0yMjJgbm5eWrEQERGVGXYT6Kb3bILc3FwsWLAANWrUgJWVFe7evQsAmDVrFr755ptSD5CIiKg0lGQp4pIOPqzo9E4GFi5ciE2bNmHRokVaL0Zo2LAhvv7661INjoiIiMqe3snAli1b8NVXX2Ho0KEwMTHR7G/SpAlu3LhRqsERERGVlvxugpJsxkrvMQOPHj2Cu7t7gf1qtVprOUQiIqKKhAMIddO7MuDt7V3o4ga7d+/WrLJERERElYfelYHZs2cjICAAjx49glqtxp49exAdHY0tW7bgwIEDZREjERFRiUn+3kpyvLHSuzLQp08f7N+/H8eOHUOVKlUwe/ZsXL9+Hfv370eXLl3KIkYiIqIS42wC3Yq1zkC7du0QGhpa2rEQERGRARR70aGLFy/i+vXrAPLGEbRo0aLUgiIiIiptxXkN8cvHGyu9k4GHDx9i8ODBOHv2LGxsbAAAycnJeP3117Fjxw7UrFmztGMkIiIqsfJ+a2FloveYgTFjxiA7OxvXr19HUlISkpKScP36dajVaowZM6YsYiQiIqIypHdl4NSpUzh37hw8PT01+zw9PbFq1Sq0a9euVIMjIiIqTUb8y32J6J0MuLi4FLq4UG5uLpydnUslKCIiotLGbgLd9O4mWLx4MSZMmICLFy9q9l28eBETJ07EF198UarBERERlZb8AYQl2YxVkSoDVatW1cqI0tPT0aZNG5ia5h2ek5MDU1NTjBo1Cn379i2TQImIiKhsFCkZWL58eRmHQUREVLbYTaBbkZKBgICAso6DiIioTHE5Yt2KvegQAGRkZCArK0trn0KhKFFAREREVL70TgbS09Mxffp07Nq1C4mJiQW+z83NLZXAiIiIShNfYayb3rMJpk2bhuPHj2Pt2rWQy+X4+uuvMW/ePDg7O2PLli1lESMREVGJSSQl34yV3pWB/fv3Y8uWLejYsSNGjhyJdu3awd3dHa6urti2bRuGDh1aFnESERFRGdG7MpCUlIQ6deoAyBsfkJSUBAB44403cPr06dKNjoiIqJTwFca66Z0M1KlTBzExMQCA+vXrY9euXQDyKgb5Ly4iIiKqaNhNoJveycDIkSNx6dIlAMBHH32E1atXw9zcHJMnT8bUqVNLPUAiIiIqW3qPGZg8ebLm3319fXHjxg1ERETA3d0djRs3LtXgiIiISgtnE+hWonUGAMDV1RWurq6lEQsREVGZKWmp34hzgaIlAytXrizyCT/44INiB0NERFRWuByxbkVKBpYtW1akk0kkEiYDRERElUyRkoH82QMVVXPnqrCy5jLI5eFmXKqhQxAdH/dqhg6ByChIUYxR8y8db6xKPGaAiIioMmA3gW7GnOgQERFREbAyQEREoiCRAFLOJigUkwEiIhIFaQmTgZIcW9Gxm4CIiEjkipUM/Prrr3j33Xfh4+ODR48eAQC2bt2KM2fOlGpwREREpYUvKtJN72Tgxx9/hJ+fHywsLPDnn38iMzMTAJCSkoLPPvus1AMkIiIqDfndBCXZjJXeycCnn36KdevWYcOGDTAzM9Psb9u2Lf74449SDY6IiIjKnt4DCKOjo9G+ffsC+5VKJZKTk0sjJiIiolLHdxPopndlwNHREbdv3y6w/8yZM6hTp06pBEVERFTa8t9aWJLNWOmdDIwdOxYTJ07Eb7/9BolEgsePH2Pbtm2YMmUKxo0bVxYxEhERlZi0FDZjpXc3wUcffQS1Wo3OnTvj+fPnaN++PeRyOaZMmYIJEyaURYxERERUhvROBiQSCT755BNMnToVt2/fRlpaGry9vWFlZVUW8REREZUKjhnQrdgrEMpkMnh7e5dmLERERGVGipL1+0thvNmA3slAp06d/nPhhePHj5coICIiIipfeicDTZs21fqcnZ2NyMhIXLlyBQEBAaUVFxERUaliN4FueicDy5YtK3T/3LlzkZaWVuKAiIiIygJfVKRbqc2UePfdd/Htt9+W1umIiIionJTaK4zDw8Nhbm5eWqcjIiIqVRIJSjSAkN0E/9KvXz+tz4IgIDY2FhcvXsSsWbNKLTAiIqLSxDEDuumdDCiVSq3PUqkUnp6emD9/Prp27VpqgREREVH50CsZyM3NxciRI9GoUSNUrVq1rGIiIiIqdRxAqJteAwhNTEzQtWtXvp2QiIgqHUkp/GOs9J5N0LBhQ9y9e7csYiEiIioz+ZWBkmzGSu9k4NNPP8WUKVNw4MABxMbGQqVSaW1ERERUuRR5zMD8+fPx4Ycfonv37gCA3r17ay1LLAgCJBIJcnNzSz9KIiKiEuKYAd2KnAzMmzcP77//Pk6cOFGW8RAREZUJiUTyn+/WKcrxxqrIyYAgCACADh06lFkwREREVP70mlpozFkREREZN3YT6KZXMuDh4fHKhCApKalEAREREZUFrkCom17JwLx58wqsQEhEREQFnT59GosXL0ZERARiY2Oxd+9e9O3bV/O9IAiYM2cONmzYgOTkZLRt2xZr165FvXr1NG2SkpIwYcIE7N+/H1KpFP3798eKFStgZWWlaXP58mUEBgbiwoULqF69OiZMmIBp06bpFateycCgQYNgb2+v1wWIiIgqAqlEUqIXFel7bHp6Opo0aYJRo0YVeK8PACxatAgrV67E5s2b4ebmhlmzZsHPzw/Xrl3TvPhv6NChiI2NRWhoKLKzszFy5Ei899572L59OwBApVKha9eu8PX1xbp16xAVFYVRo0bBxsYG7733XpFjLXIywPECRERUmZX3mAF/f3/4+/sX+p0gCFi+fDlmzpyJPn36AAC2bNkCBwcH7Nu3D4MGDcL169dx+PBhXLhwAS1btgQArFq1Ct27d8cXX3wBZ2dnbNu2DVlZWfj2228hk8nQoEEDREZGYunSpXolA0VedCh/NgERERGVTExMDOLi4uDr66vZp1Qq0aZNG4SHhwMAwsPDYWNjo0kEAMDX1xdSqRS//fabpk379u0hk8k0bfz8/BAdHY1nz54VOZ4iVwbUanWRT0pERFThlHAAYf6rCV5ebVcul0Mul+t1qri4OACAg4OD1n4HBwfNd3FxcQW65k1NTWFra6vVxs3NrcA58r8r6ksF9V6OmIiIqDKSQlLiDQBcXFygVCo1W0hIiIHvrOT0GkBIRERUWZXW1MIHDx5AoVBo9utbFQAAR0dHAEB8fDycnJw0++Pj49G0aVNNm4SEBK3jcnJykJSUpDne0dER8fHxWm3yP+e3KQpWBoiIiPSgUCi0tuIkA25ubnB0dERYWJhmn0qlwm+//QYfHx8AgI+PD5KTkxEREaFpc/z4cajVarRp00bT5vTp08jOzta0CQ0NhaenZ5G7CAAmA0REJBLl/QrjtLQ0REZGIjIyEkDeoMHIyEjcv38fEokEkyZNwqeffoqff/4ZUVFRGD58OJydnTVrEXh5eaFbt24YO3Ysfv/9d5w9exZBQUEYNGgQnJ2dAQBDhgyBTCbD6NGjcfXqVezcuRMrVqxAcHCwXrGym6CU7DwQjl0HwvE4IW/0Zt1aDvjfUF+0a1Vf0+bStb+wcvNhRN24DxMTKTzrOGPdwjEwl5sBALoND9Ecn2/iSH+MHtip/G6kknmapMLG7aG4eOkWMjOz4eRoi8n/6wuPujUAAC8yMrHx+2MIv3gDqanP4WBfFb392qBHl1aac0yfvxFR1+9pnde/c0tMGNOrPG/FqGzYdQqrvgtDQqIKDevVwOdT30GLBrUNHZbR4vMumvJeZ+DixYvo1Omfv7/zf0AHBARg06ZNmDZtGtLT0/Hee+8hOTkZb7zxBg4fPqxZYwAAtm3bhqCgIHTu3Fmz6NDKlSs13yuVShw9ehSBgYFo0aIF7OzsMHv2bL2mFQKARKjEcwZVKhWUSiX+uBULK2vFqw8oQyfPX4OJVIJaNewgCMDPxyKwafcp7PpyItxrO+LStb8wbuY3GD2wEzq08YKJiRQ3Y2LR6bUGkMnycrJuw0Pwll8r9PdvozmvpaUcluYyXZctdw+fvTB0CBqpaS8wYcY6NG5QGz18W0GpqILHcYlwcrCFk4MtAGDlhp9x6WoMJr7XGw7VbfDH5TtY/e1BzJw8EK+1zEvUps/fiBpO1fDuO//8T2suM4OlpXmh1y1vPu7VDB2CXvYcjcC4uVux9KOBaNGwNtZ9fwL7wv7Ehd2zUd3W2tDhGZ3K/rxVKhUcqimRkpKi1Q9f2tdQKpVYfiwKFlWK/0xepKdikm+jMo3VUAzaTRASEoJWrVrB2toa9vb26Nu3L6Kjow0ZUrF1fM0b7Vp7wbVGddSuWR0fjOgGS3MZLt+4DwBY9NV+DOnTFqMHdoJ7bUe4udjDr30TTSKQz9JSDjtba81WkRKBimb3/jOoXk2B4Pffgqd7TTjaV0Xzxu6aRAAArt98gM7tm6CxtxscqleFf+eWqOPqgOg7j7TOJZeZwdbGWrNVlESgMlqz/TiG930dQ3v7oH4dJyydMQiW5jJ893O4oUMzSnzeRZc/gLAkm7EyaDJw6tQpBAYG4vz585qlFrt27Yr09HRDhlViublqHDoZiReZWWji5YrE5DRE3bgPWxsrDJu8Gh0HzcfIqWvxx5WYAsd+u+sE2r0zFwMCl2PjDyeRk5trgDuoHM5HRKNeHWd8tnwnBv9vEYI+WovDYRe12nh5uOC3iGg8TVJBEARcuhqDR7GJaN64rla7E2cvY9DYzzFu6mps/D4UGZlZ5XkrRiMrOweRNx6gY2tPzT6pVIoOrT1xIargn3cqGT5v/Ugh0XQVFGuD8WYDBh0zcPjwYa3PmzZtgr29PSIiItC+fXsDRVV8N2NiMWzyamRl5cDSQobls4ajrqsDLl3/CwCw9rtQfDi2BzzrOGN/WATGzvgKe9YFw7VGdQDAkD5t4eVeA0prS0Rev4cVGw/jaVIqpv6PfdeFiUt4hoPHLuKt7j4Y2Kc9bt59hHWbD8HU1BS+HZoCAMaN6I6VG37G8MAlMDGRQiKRYOLY3mjkVVtzno5tG8Hezga2Va1x7348vv0+FI9iEzEzeJBhbqwSS0xOQ26uukB5urqtArfuxes4ioqLz5tKS4UaQJiSkgIAsLW1LfT7zMxMZGZmaj6/vAqUobnVrI4f1kxCWnoGQn+Nwswlu/Dtovc1Szm/3b0N+nbNG7jm5V4Dv/15G/uOXMTEUXlrVw/v/08C5FHHCWampliw8kdMHOlfoDuBAEEtoF4dZ4wYlLecZ103J/z1IAG/hF3QJAM/H/kNN24/xJwpQ2Bvp8SVG39hzcaDsK1qjWaN8qoD/p3/WerTrZYDqtpY4eOFmxEbn6TV5UBElRtfYaxbhZlaqFarMWnSJLRt2xYNGzYstE1ISIjWqk8uLi7lHOV/MzMzRS1nO3jXq4mJo/zh4eaEbfvOwM42b6BJ3Vray07WqWWP2Ce6145u5OmCnFw1HsUnlWnclVXVqlZwqVlda59LDTs8eZqXVGZmZWPzjjCMfbcb2rTwhJurI3r5tUE7n4bYc+CczvPWd68JAHgcx+eur2o2VjAxkeJJUqrW/idJKthXM64BVxUBn7d+pKWwGasKc2+BgYG4cuUKduzYobPNjBkzkJKSotkePHhQjhHqTy0IyMrOQQ2HqrCvpsC9h0+0vv/r0VM42eteFCL67mNIpRJUs7HS2UbMvD1q4dHjp1r7HsUmwt7OBgCQm5OLnNxcSF6aHGwilUD9H5No7vyVt+a3LZ+73mRmpmha3wWnLvwzEFitVuP0hZto1cjtP46k4uDzptJSIWrPQUFBOHDgAE6fPo2aNWvqbFecl0GUlxXfHkLbVp5wqm6D9BeZOHQiEhcv38W6haMhkUgQ8HYHrN0aCo86Tqhf1xk/h0Yg5kEClnwyDEDeGgSXo++jdZO6qGIhx6Xrf2HR+v3o8WZzKKwtDXx3FdNb3X3w4ZyvsXPfabR7rQGi7zzCoeMR+GBMbwCApaU5GnnVxrfbjkIuM4W9nQ2irt9D2OlLGDvMDwAQG5+EE2cvo1VTDyisLRDzVzy+2noYDeu7ws216Et50j/GD3kT4+dtRTOvWmjeoDbWfn8C6S8yMbTXa4YOzSjxeRedRCKBpAS1/pIcW9EZNBkQBAETJkzA3r17cfLkyQJvXqpMkpLTMHPxTjx5poKVpTk83JywbuFo+DT3AAAMe6sdsrJysHj9fqSkPodnHWes/2wsXJzz5pCbmZng8KlLWPddaF41wdEWw95qh+H9Kt9AyvLiUbcGZgYPwqYdx7B9zyk4VrfB/4Z1Q6c3GmvaTP/gbWzacQyLv/wRqWkvYF/dBsMHdkZ337yxG6amJoiMuoufDp1HRmY2qldToG1rbwx+i8+9uPp1bYGnyWn4bP1BJCSmopFHDexeGciydRnh8y46CVCi+QDGmwoYeNGh8ePHY/v27fjpp5/g6fnP1BilUgkLC4tXHl+RFh0Si4q06JBYVLZFh4j0UZ6LDn118hosrEqw6FBaKt7r6M1Fh0rb2rVrkZKSgo4dO8LJyUmz7dy505BhERERiYrBuwmIiIjKizGX+kuiQgwgJCIiKmtcZ0C3CjO1kIiIiAyDlQEiIhIFTi3UjckAERGJQklXETTmUrox3xsREREVASsDREQkCuwm0I3JABERiQJXINSN3QREREQix8oAERGJArsJdGMyQEREosDZBLoxGSAiIlFgZUA3Y050iIiIqAhYGSAiIlHgbALdmAwQEZEo8EVFurGbgIiISORYGSAiIlGQQgJpCYr9JTm2omMyQEREosBuAt3YTUBERCRyrAwQEZEoSP7+pyTHGysmA0REJArsJtCN3QREREQix8oAERGJgqSEswnYTUBERFTJsZtANyYDREQkCkwGdOOYASIiIpFjZYCIiESBUwt1YzJARESiIJXkbSU53lixm4CIiEjkWBkgIiJRYDeBbkwGiIhIFDibQDd2ExAREYkcKwNERCQKEpSs1G/EhQEmA0REJA6cTaAbuwmIiIhEjpUBIiISBc4m0I3JABERiQJnE+jGZICIiERBgpINAjTiXIBjBoiIiMSOlQEiIhIFKSSQlqDWLzXi2oBRJANONhZQKCwMHYYomJqwmERElRO7CXTj3+xEREQiZxSVASIioldiaUAnJgNERCQKXGdAN3YTEBERiRwrA0REJA4lXHTIiAsDTAaIiEgcOGRAN3YTEBERiRwrA0REJA4sDejEZICIiESBswl0YzJARESiwLcW6sYxA0RERCLHygAREYkChwzoxmSAiIjEgdmATuwmICIiKgNz586FRCLR2urXr6/5PiMjA4GBgahWrRqsrKzQv39/xMfHa53j/v376NGjBywtLWFvb4+pU6ciJyen1GNlZYCIiETBELMJGjRogGPHjmk+m5r+82N38uTJOHjwIH744QcolUoEBQWhX79+OHv2LAAgNzcXPXr0gKOjI86dO4fY2FgMHz4cZmZm+Oyzz4p9H4VhMkBERKJgiNkEpqamcHR0LLA/JSUF33zzDbZv344333wTALBx40Z4eXnh/PnzeO2113D06FFcu3YNx44dg4ODA5o2bYoFCxZg+vTpmDt3LmQyWfFv5iXsJiAiItKDSqXS2jIzM3W2vXXrFpydnVGnTh0MHToU9+/fBwBEREQgOzsbvr6+mrb169dHrVq1EB4eDgAIDw9Ho0aN4ODgoGnj5+cHlUqFq1evluo9MRkgIiJRkJTCBgAuLi5QKpWaLSQkpNDrtWnTBps2bcLhw4exdu1axMTEoF27dkhNTUVcXBxkMhlsbGy0jnFwcEBcXBwAIC4uTisRyP8+/7vSxG4CIiISh1KaTfDgwQMoFArNbrlcXmhzf39/zb83btwYbdq0gaurK3bt2gULC4sSBFL6WBkgIiLSg0Kh0Np0JQMvs7GxgYeHB27fvg1HR0dkZWUhOTlZq018fLxmjIGjo2OB2QX5nwsbh1ASTAaIiEgUJKXwT0mkpaXhzp07cHJyQosWLWBmZoawsDDN99HR0bh//z58fHwAAD4+PoiKikJCQoKmTWhoKBQKBby9vUsUy8vYTUBERKJQ3rMJpkyZgl69esHV1RWPHz/GnDlzYGJigsGDB0OpVGL06NEIDg6Gra0tFAoFJkyYAB8fH7z22msAgK5du8Lb2xvDhg3DokWLEBcXh5kzZyIwMLDI1YiiYjJARESiUN4LED58+BCDBw9GYmIiqlevjjfeeAPnz59H9erVAQDLli2DVCpF//79kZmZCT8/P6xZs0ZzvImJCQ4cOIBx48bBx8cHVapUQUBAAObPn1+CuyicRBAEodTPWk5UKhWUSiVinyRrDeagshOv0j2FhsqGk425oUMgKjMqlQoO1ZRISUkps7/H839WhF97BCvr4l8jLVUFH+8aZRqrobAyQERE4sB3E+jEZICIiETBEMsRVxacTUBERCRyrAwQEZEoGOLdBJUFkwEiIhIFDhnQjd0EREREIsfKABERiQNLAzoxGSAiIlHgbALd2E1AREQkcqwMEBGRKHA2gW5MBoiISBQ4ZEA3JgNERCQOzAZ04pgBIiIikWNlgIiIRIGzCXRjMkBEROJQwgGERpwLsJuAiIhI7FgZKEPLNh3FgZOXcOuveFjIzdCqkRvmBPVBPVeHAm0FQcDAyWsRFn4dWxaNQY8OTQwQceWyY/857NgfjkfxSQAAd1dHjHvXF+1bewEAMrOysWjdfvxyMhJZ2Tl4o6UnZn3QD3ZVrTXn8O4ypcB5v/h4KLp3alY+N2Fkzv5xG6u2HsOlG/cR91SF7xaPRY+O/LNc1jbsOoVV34UhIVGFhvVq4POp76BFg9qGDqvC4fhB3QxaGVi7di0aN24MhUIBhUIBHx8fHDp0yJAhlapzf97G6Lfb4eg3H+LHlYHIycnF2x+sRvqLzAJt1+04YdT9UWXBwU6JyaO744fVk/DD6klo09QdQXM24da9OADA/639GSfOX8OyWcOwZcl4JCSqMHHu5gLnWThlIE7tnK3ZOrdtWN63YjSev8hEQ48aWDxtoKFDEY09RyMwc/leTB/jj5Nbp6NhvRroP2E1niSlGjq0ikdSCpuRMmgyULNmTfzf//0fIiIicPHiRbz55pvo06cPrl69asiwSs0PK8ZjSM/XUL+OExp61MSXs9/Fw7hnuHTjgVa7qJsPsXrbCaycNdRAkVZOnXwaoEMbL9SuWR21a1bHpFH+sLSQ4fL1v5Ca/gI/Hv4d09/vhdea1UMDj5pYOGUg/rx2D5eu/aV1HmsrC1S3VWg2uczMQHdU+XVp2wAzx/VCz06sBpSXNduPY3jf1zG0tw/q13HC0hmDYGkuw3c/hxs6NKpEDJoM9OrVC927d0e9evXg4eGBhQsXwsrKCufPnzdkWGVGlZYBAKiqsNTse56Rhfdmbcaiqe/AoZrCUKFVerm5avxy4k+8yMhCE29XXL35EDk5ufBp7qFpU6eWPZzsbRB5XTsZ+HTVHrzefzYGBq3Aj4d/hyAI5R0+UbFkZecg8sYDdGztqdknlUrRobUnLkTFGDCyiklSCv8YqwozZiA3Nxc//PAD0tPT4ePjY+hwSp1arcYny35Em8Z14FXXWbN/5rI9aN3YDd07NDZgdJXXzZhYDP5gFbKycmBpIcPKOSPg7uqIG3cew8zMBAorC632dlWt8TRJpfk8IcAPbZq6w9xchnMXo7Fg5R48f5GJYW+1K+9bIdJbYnIacnPVqG5rrbW/uq0Ct+7FGyiqiovLEetm8GQgKioKPj4+yMjIgJWVFfbu3Qtvb+9C22ZmZiIz85/+dpVKVWi7imjq4h9w/W4sDq6fpNl36HQUfr14Eye2TjdcYJVc7ZrVsWddMNLSM3Dk18v4ePEObF4yrsjHj3u3i+bfvd1r4EVGFjb+cJLJABGJisGnFnp6eiIyMhK//fYbxo0bh4CAAFy7dq3QtiEhIVAqlZrNxcWlnKMtnmmLd+HomSv4ac0E1HCoqtn/68WbiHn0FHV8p8H+9Ymwf30iAGDER9+g97gVhgq3UpGZmcK1hh0aeNRE8Oju8KzjjK17z8CuqjWys3OhSnuh1f7ps1TY2erujmnsVQtxT1KQlZVT1qETlVg1GyuYmEgLDBZ8kqSCPbsdC+D4Qd0MXhmQyWRwd3cHALRo0QIXLlzAihUrsH79+gJtZ8yYgeDgYM1nlUpVoRMCQRAw/YsfcPDUZfy85gO4OttpfT8xoAuG9dHuEnljSAg+ndQP3dpxRHtxCIIa2Vk5aOBRE6amJjj/5y10bZfXBRPzIAGxCclo6uWq8/jrtx9DYW0Bmczg/2sQvZLMzBRN67vg1IVozRROtVqN0xduYsw77Q0cXQXEuYU6Vbi/8dRqtVZXwL/J5XLI5fJyjqj4pi7ehR+PROC7xWNhVcUc8Yl53RqKKuawMJfBoZqi0EGDNR2rFkgcqKCl3/yC9q084WRfFekvMnHg+J/4/dJdbAgZC+sqFujfrTU+X/czlNaWsLI0x8LVe9HU2xVNvPOSgRPhV5H4LA1NvGpBJjND+B83sWFHGEa83dGwN1aJpT3PRMyDJ5rPfz1ORFT0Q9goLeHiaGvAyIzX+CFvYvy8rWjmVQvNG9TG2u9PIP1FJob2es3QoVU4XI5YN4MmAzNmzIC/vz9q1aqF1NRUbN++HSdPnsSRI0cMGVap2fjjGQBA73ErtfavmjUUQ3ryf9SSSkpOw0eLduBJkgrWVczh4eaMDSFj8XqLvBkEH43rDalEgonzNyM7OwdtW+QtOpTP1NQE238+i/9b9zMEQUAtZztM+19vvNO9jaFuqdKLvP4Xer3/z5/3T5btAQAM7tEGa+YOM1RYRq1f1xZ4mpyGz9YfREJiKhp51MDulYHsJiC9SAQDzqMaPXo0wsLCEBsbC6VSicaNG2P69Ono0qXLqw9GXjeBUqlE7JNkKBT8g18e4lWFV22o7DjZmBs6BKIyo1Kp4FBNiZSUlDL7ezz/Z8WVmARYl+AaqSoVGrrZl2mshmLQysA333xjyMsTEZGIcMiAbgafTUBERESGVeEGEBIREZUFLjqkG5MBIiISCXYU6MJuAiIiIpFjZYCIiESB3QS6MRkgIiJRYCeBbuwmICIiEjlWBoiISBTYTaAbkwEiIhIFvptANyYDREQkDhw0oBPHDBAREYkcKwNERCQKLAzoxmSAiIhEgQMIdWM3ARERkcixMkBERKLA2QS6MRkgIiJx4KABndhNQEREJHKsDBARkSiwMKAbkwEiIhIFzibQjd0EREREIsfKABERiUTJZhMYc0cBkwEiIhIFdhPoxm4CIiIikWMyQEREJHLsJiAiIlFgN4FuTAaIiEgUuByxbuwmICIiEjlWBoiISBTYTaAbkwEiIhIFLkesG7sJiIiIRI6VASIiEgeWBnRiMkBERKLA2QS6sZuAiIhI5FgZICIiUeBsAt2YDBARkShwyIBu7CYgIiJxkJTCVgyrV69G7dq1YW5ujjZt2uD3338v2X2UASYDREREZWTnzp0IDg7GnDlz8Mcff6BJkybw8/NDQkKCoUPTwmSAiIhEQVIK/+hr6dKlGDt2LEaOHAlvb2+sW7cOlpaW+Pbbb8vgDouPyQAREYlC/gDCkmz6yMrKQkREBHx9fTX7pFIpfH19ER4eXsp3VzKVegChIAgAgNRUlYEjEY/U1ExDhyA6VaRZhg6BqMykqvL+/s7/+7wsqVQl+1mRf/zL55HL5ZDL5QXaP336FLm5uXBwcNDa7+DggBs3bpQoltJWqZOB1NRUAIBHnVoGjoSIiEoiNTUVSqWyTM4tk8ng6OiIem4uJT6XlZUVXFy0zzNnzhzMnTu3xOc2pEqdDDg7O+PBgwewtraGpBJNAFWpVHBxccGDBw+gUCgMHY4o8JmXLz7v8ldZn7kgCEhNTYWzs3OZXcPc3BwxMTHIyip5lU0QhAI/bwqrCgCAnZ0dTExMEB8fr7U/Pj4ejo6OJY6lNFXqZEAqlaJmzZqGDqPYFApFpfqf1hjwmZcvPu/yVxmfeVlVBP7N3Nwc5ubmZX6df5PJZGjRogXCwsLQt29fAIBarUZYWBiCgoLKNZZXqdTJABERUUUWHByMgIAAtGzZEq1bt8by5cuRnp6OkSNHGjo0LUwGiIiIysjAgQPx5MkTzJ49G3FxcWjatCkOHz5cYFChoTEZMAC5XI45c+bo7Gei0sdnXr74vMsfn3nFFRQUVOG6BV4mEcpjPgcRERFVWFx0iIiISOSYDBAREYkckwEiIiKRYzJAREQkckwGDKAyvNvaWJw+fRq9evWCs7MzJBIJ9u3bZ+iQjFpISAhatWoFa2tr2Nvbo2/fvoiOjjZ0WEZr7dq1aNy4sWahIR8fHxw6dMjQYVElxGSgnFWWd1sbi/T0dDRp0gSrV682dCiicOrUKQQGBuL8+fMIDQ1FdnY2unbtivT0dEOHZpRq1qyJ//u//0NERAQuXryIN998E3369MHVq1cNHRpVMpxaWM7atGmDVq1a4csvvwSQtzSli4sLJkyYgI8++sjA0Rk3iUSCvXv3apYFpbL35MkT2Nvb49SpU2jfvr2hwxEFW1tbLF68GKNHjzZ0KFSJsDJQjirTu62JSkNKSgqAvB9QVLZyc3OxY8cOpKenw8fHx9DhUCXDFQjLUWV6tzVRSanVakyaNAlt27ZFw4YNDR2O0YqKioKPjw8yMjJgZWWFvXv3wtvb29BhUSXDZICIykRgYCCuXLmCM2fOGDoUo+bp6YnIyEikpKRg9+7dCAgIwKlTp5gQkF6YDJSjyvRua6KSCAoKwoEDB3D69OlK/ZrxykAmk8Hd3R0A0KJFC1y4cAErVqzA+vXrDRwZVSYcM1CO/v1u63z577ZmHx8ZA0EQEBQUhL179+L48eNwc3MzdEiio1arkZmZaegwqJJhZaCcVZZ3WxuLtLQ03L59W/M5JiYGkZGRsLW1Ra1atQwYmXEKDAzE9u3b8dNPP8Ha2hpxcXEAAKVSCQsLCwNHZ3xmzJgBf39/1KpVC6mpqdi+fTtOnjyJI0eOGDo0qmQ4tdAAvvzySyxevFjzbuuVK1eiTZs2hg7LKJ08eRKdOnUqsD8gIACbNm0q/4CMnEQiKXT/xo0bMWLEiPINRgRGjx6NsLAwxMbGQqlUonHjxpg+fTq6dOli6NCokmEyQEREJHIcM0BERCRyTAaIiIhEjskAERGRyDEZICIiEjkmA0RERCLHZICIiEjkmAwQERGJHJMBohIaMWIE+vbtq/ncsWNHTJo0qdzjOHnyJCQSCZKTk3W2kUgk2LdvX5HPOXfuXDRt2rREcd27dw8SiQSRkZElOg8RlR0mA2SURowYAYlEAolEonmRy/z585GTk1Pm196zZw8WLFhQpLZF+QFORFTW+G4CMlrdunXDxo0bkZmZiV9++QWBgYEwMzPDjBkzCrTNysqCTCYrleva2tqWynmIiMoLKwNktORyORwdHeHq6opx48bB19cXP//8M4B/SvsLFy6Es7MzPD09AQAPHjzAgAEDYGNjA1tbW/Tp0wf37t3TnDM3NxfBwcGwsbFBtWrVMG3aNLy8ovfL3QSZmZmYPn06XFxcIJfL4e7ujm+++Qb37t3TvDehatWqkEgkmvX71Wo1QkJC4ObmBgsLCzRp0gS7d+/Wus4vv/wCDw8PWFhYoFOnTlpxFtX06dPh4eEBS0tL1KlTB7NmzUJ2dnaBduvXr4eLiwssLS0xYMAApKSkaH3/9ddfw8vLC+bm5qhfvz7WrFmjdyxEZDhMBkg0LCwskJWVpfkcFhaG6OhohIaG4sCBA8jOzoafnx+sra3x66+/4uzZs7CyskK3bt00xy1ZsgSbNm3Ct99+izNnziApKQl79+79z+sOHz4c33//PVauXInr169j/fr1sLKygouLC3788UcAQHR0NGJjY7FixQoAQEhICLZs2YJ169bh6tWrmDx5Mt59912cOnUKQF7S0q9fP/Tq1QuRkZEYM2YMPvroI72fibW1NTZt2oRr165hxYoV2LBhA5YtW6bV5vbt29i1axf279+Pw4cP488//8T48eM132/btg2zZ8/GwoULcf36dXz22WeYNWsWNm/erHc8RGQgApERCggIEPr06SMIgiCo1WohNDRUkMvlwpQpUzTfOzg4CJmZmZpjtm7dKnh6egpqtVqzLzMzU7CwsBCOHDkiCIIgODk5CYsWLdJ8n52dLdSsWVNzLUEQhA4dOggTJ04UBEEQoqOjBQBCaGhooXGeOHFCACA8e/ZMsy8jI0OwtLQUzp07p9V29OjRwuDBgwVBEIQZM2YI3t7eWt9Pnz69wLleBkDYu3evzu8XL14stGjRQvN5zpw5gomJifDw4UPNvkOHDglSqVSIjY0VBEEQ6tatK2zfvl3rPAsWLBB8fHwEQRCEmJgYAYDw559/6rwuERkWxwyQ0Tpw4ACsrKyQnZ0NtVqNIUOGYO7cuZrvGzVqpDVO4NKlS7h9+zasra21zpORkYE7d+4gJSUFsbGxWq+bNjU1RcuWLQt0FeSLjIyEiYkJOnToUOS4b9++jefPnxd4DW1WVhaaNWsGALh+/XqB1177+PgU+Rr5du7ciZUrV+LOnTtIS0tDTk4OFAqFVptatWqhRo0aWtdRq9WIjo6GtbU17ty5g9GjR2Ps2LGaNjk5OVAqlXrHQ0SGwWSAjFanTp2wdu1ayGQyODs7w9RU+497lSpVtD6npaWhRYsW2LZtW4FzVa9evVgxWFhY6H1MWloaAODgwYNaP4SBvHEQpSU8PBxDhw7FvHnz4OfnB6VSiR07dmDJkiV6x7phw4YCyYmJiUmpxUpEZYvJABmtKlWqwN3dvcjtmzdvjp07d8Le3r7Ab8f5nJyc8Ntvv6F9+/YA8n4DjoiIQPPmzQtt36hRI6jVapw6dQq+vr4Fvs+vTOTm5mr2eXt7Qy6X4/79+zorCl5eXprBkPnOnz//6pv8l3PnzsHV1RWffPKJZt9ff/1VoN39+/fx+PFjODs7a64jlUrh6ekJBwcHODs74+7duxg6dKhe1yeiioMDCIn+NnToUNjZ2aFPnz749ddfERMTg5MnT+KDDz7Aw4cPAQATJ07E//3f/2Hfvn24ceMGxo8f/59rBNSuXRsBAQEYNWoU9u3bpznnrl27AACurq6QSCQ4cOAAnjx5grS0NFhbW2PKlCmYPHkyNm/ejDt37uCPP/7AqlWrNIPy3n//fdy6dQtTp05FdHQ0tm/fjk2bNul1v/Xq1cP9+/exY8cO3LlzBytXrix0MKS5uTkCAgJw6dIl/Prrr/jggw8wYMAAODo6AgDmzZuHkJAQrFy5Ejdv3kRUVBQ2btyIpUuX6hUPERkOkwGiv1laWuL06dOoVasW+vXrBy8vL4wePRoZGRmaSsGHH36IYcOGISAgAD4+PrC2tsZbb731n+ddu3Yt3n77bYwfPx7169fH2LFjkZ6eDgCoUaMG5s2bh48++ggODg4ICgoCACxYsACzZs1CSEgIvLy80K1bNxw8eBBubm4A8vrxf/zxR+zbtw9NmjTBunXr8Nlnn+l1v71798bkyZMRFBSEpk2b4ty5c5g1a1aBdu7u7ujXrx+6d++Orl27onHjxlpTB8eMGYOvv/4aGzduRKNGjdChQwds2rRJEysRVXwSQdfIJyIiIhIFVgaIiIhEjskAERGRyDEZICIiEjkmA0RERCLHZICIiEjkmAwQERGJHJMBIiIikWMyQEREJHJMBoiIiESOyQAREZHIMRkgIiISOSYDREREIvf/Ha3WW0d9JWYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# 1. 데이터 전처리 및 로더 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".bin\"):\n",
    "                    bin_path = os.path.join(root, file)\n",
    "                    json_path = bin_path.replace(\".bin\", \".json\")\n",
    "\n",
    "                    try:\n",
    "                        # BIN 파일 읽기 (120x160 이미지)\n",
    "                        bin_data = np.load(bin_path, allow_pickle=True).astype(np.float32)\n",
    "                        bin_data = bin_data.reshape((120, 160))\n",
    "                    except (UnicodeDecodeError, ValueError):\n",
    "                        print(f\"[Error] BIN 파일 읽기 실패: {bin_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # JSON 파일에서 state 값 읽기\n",
    "                    if os.path.exists(json_path):\n",
    "                        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            state = json_data.get(\"annotations\", [{}])[0].get(\"tagging\", [{}])[0].get(\"state\", \"N/A\")\n",
    "                    else:\n",
    "                        print(f\"[Warning] JSON 파일을 찾을 수 없습니다: {json_path}\")\n",
    "                        continue\n",
    "\n",
    "                    self.data.append(bin_data)\n",
    "                    self.labels.append(state)\n",
    "\n",
    "        # Label Encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        print(f\"폴더: {root_folder}, 데이터 개수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # PyTorch 텐서 변환 및 채널 추가\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # (1, 120, 160)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size=32):\n",
    "    train_dataset = CustomDataset(os.path.join(data_folder, \"train\"))\n",
    "    val_dataset = CustomDataset(os.path.join(data_folder, \"val\"))\n",
    "    test_dataset = CustomDataset(os.path.join(data_folder, \"test\"))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 2. 모델 정의\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vit = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate  # Transformer 내부 드롭아웃 적용\n",
    "        )\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_dim_h // patch_size) * (img_dim_w // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # 추가 드롭아웃 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x).flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        x = patches + self.pos_embedding\n",
    "        x = self.vit(x, x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.dropout(x.mean(dim=1))  # 드롭아웃 적용\n",
    "        return x  # [batch_size, embed_dim]\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, num_classes, dropout_rate=0.5):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate  # 전달\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Classifier 레이어에도 드롭아웃 추가\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# 4. 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Append labels and predictions for confusion matrix\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return test_loss / len(test_loader), test_accuracy\n",
    "\n",
    "\n",
    "# 5. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\"\n",
    "    batch_size = 16\n",
    "    train_loader, val_loader, test_loader = load_data(data_folder, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ViTClassifier(\n",
    "        img_dim_h=120, img_dim_w=160, patch_size=16, embed_dim=128,\n",
    "        num_heads=4, depth=8, num_classes=4, dropout_rate=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "\n",
    "    # Test the model and plot confusion matrix\n",
    "    test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e7453-f6cb-4c8f-b8f1-23a87e471f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308b921-b0fc-4b2b-ab31-56e92a703830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4565f-585a-4e34-996b-54e81ed73425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f07956-c796-4e6f-887d-2ffa63c4e5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb37712-7ab4-4d7b-bfe6-1f95d67d1b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81aaf8-eb30-42c5-826a-d0bae5acf2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba42717-3715-41e3-99c6-a6569e6515e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62a184c-d6a2-451e-b229-d217bcdebe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\train, 데이터 개수: 35375\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\val, 데이터 개수: 5052\n",
      "폴더: C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\\test, 데이터 개수: 5052\n",
      "Using device: cuda\n",
      "Epoch 1/30, Train Loss: 1.1805, Val Loss: 1.1581, Val Accuracy: 53.92%\n",
      "Epoch 2/30, Train Loss: 1.1730, Val Loss: 1.1584, Val Accuracy: 53.92%\n",
      "Epoch 3/30, Train Loss: 1.1703, Val Loss: 1.1561, Val Accuracy: 53.92%\n",
      "Epoch 4/30, Train Loss: 1.1704, Val Loss: 1.1589, Val Accuracy: 53.92%\n",
      "Epoch 5/30, Train Loss: 1.1706, Val Loss: 1.1556, Val Accuracy: 53.92%\n",
      "Epoch 6/30, Train Loss: 1.1688, Val Loss: 1.1540, Val Accuracy: 53.92%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     train_losses, val_losses, val_accuracies = train_model(\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16308\\1593150746.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mpatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, num_patches, embed_dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [batch_size, num_patches, embed_dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 440\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. 데이터 전처리 및 로더 생성\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_folder, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".bin\"):\n",
    "                    bin_path = os.path.join(root, file)\n",
    "                    json_path = bin_path.replace(\".bin\", \".json\")\n",
    "\n",
    "                    try:\n",
    "                        # BIN 파일 읽기 (120x160 이미지)\n",
    "                        bin_data = np.load(bin_path, allow_pickle=True).astype(np.float32)\n",
    "                        bin_data = bin_data.reshape((120, 160))\n",
    "                    except (UnicodeDecodeError, ValueError):\n",
    "                        print(f\"[Error] BIN 파일 읽기 실패: {bin_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # JSON 파일에서 state 값 읽기\n",
    "                    if os.path.exists(json_path):\n",
    "                        with open(json_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "                            json_data = json.load(json_file)\n",
    "                            state = json_data.get(\"annotations\", [{}])[0].get(\"tagging\", [{}])[0].get(\"state\", \"N/A\")\n",
    "                    else:\n",
    "                        print(f\"[Warning] JSON 파일을 찾을 수 없습니다: {json_path}\")\n",
    "                        continue\n",
    "\n",
    "                    self.data.append(bin_data)\n",
    "                    self.labels.append(state)\n",
    "\n",
    "        # Label Encoding\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "        print(f\"폴더: {root_folder}, 데이터 개수: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # PyTorch 텐서 변환 및 채널 추가\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # (1, 120, 160)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def load_data(data_folder, batch_size=32):\n",
    "    train_dataset = CustomDataset(os.path.join(data_folder, \"train\"))\n",
    "    val_dataset = CustomDataset(os.path.join(data_folder, \"val\"))\n",
    "    test_dataset = CustomDataset(os.path.join(data_folder, \"test\"))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# 2. 모델 정의\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.vit = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=depth,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate  # Transformer 내부 드롭아웃 적용\n",
    "        )\n",
    "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (img_dim_h // patch_size) * (img_dim_w // patch_size)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # 추가 드롭아웃 레이어\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x).flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "        x = patches + self.pos_embedding\n",
    "        x = self.vit(x, x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.dropout(x.mean(dim=1))  # 드롭아웃 적용\n",
    "        return x  # [batch_size, embed_dim]\n",
    "\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, num_classes, dropout_rate=0.5):\n",
    "        super(ViTClassifier, self).__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_dim_h, img_dim_w, patch_size, embed_dim, num_heads, depth, dropout_rate  # 전달\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Classifier 레이어에도 드롭아웃 추가\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10):\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies\n",
    "\n",
    "\n",
    "# 4. 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_loss / len(test_loader), test_accuracy\n",
    "\n",
    "\n",
    "# 5. 실행\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:/Users/82103/Desktop/multimodal/train(01~14)val(15~16)test(17~18)/AGV\"\n",
    "    batch_size = 16\n",
    "    train_loader, val_loader, test_loader = load_data(data_folder, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = ViTClassifier(\n",
    "        img_dim_h=120, img_dim_w=160, patch_size=16, embed_dim=128,\n",
    "        num_heads=4, depth=8, num_classes=4, dropout_rate=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    num_epochs = 30\n",
    "    train_losses, val_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "\n",
    "    test_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11.1",
   "language": "python",
   "name": "tf37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
